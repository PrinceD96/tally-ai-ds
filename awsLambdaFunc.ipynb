{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import Session\n",
    "from lxml import html\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import logging\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import pytz\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#rds settings\n",
    "rds_host  = \"\"\n",
    "name = \"\" \n",
    "db_name = \"\" \n",
    "password = \"\"\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(host=rds_host, user=name, password=password, database=db_name, port=5432, connect_timeout=5)\n",
    "except psycopg2.OperationalError as e:\n",
    "    logger.error(\"ERROR: Unexpected error: Could not connect to postGreSQL instance.\")\n",
    "    logger.error(e)\n",
    "    sys.exit()\n",
    "\n",
    "logger.info(\"SUCCESS: Connection to RDS postGreSQL instance succeeded\")\n",
    "\n",
    "\n",
    "def yelpScrapePage(business_id, \n",
    "                   page=0, # page\n",
    "                   date_range=None # date range\n",
    "                   ): \n",
    "    ''' \n",
    "    CAUTION: Do NOT use multi-threading to avoid getting blocked.\n",
    "    '''\n",
    "    status_code, results, keep_scraping = None, [], True\n",
    "    \n",
    "    base_url = \"https://www.yelp.com/biz/\" # add business id\n",
    "    api_url = \"/review_feed?sort_by=date_desc&start=\" # add number\n",
    "\n",
    "    with Session() as s:\n",
    "        url = base_url + business_id + api_url + str(page*20)\n",
    "        with s.get(url, timeout=5) as r:    \n",
    "            status_code = r.status_code\n",
    "            if status_code != 200:\n",
    "                return status_code, results, keep_scraping\n",
    "\n",
    "            response = dict(r.json()) \n",
    "            _html = html.fromstring(response['pagination'])\n",
    "            text = _html.xpath(\"//div[@class='page-of-pages arrange_unit arrange_unit--fill']/text()\")\n",
    "            total_pages = int(text[0].strip().split(' ')[-1])\n",
    "            if page+1 > total_pages:\n",
    "                keep_scraping = False\n",
    "                return status_code, results, keep_scraping\n",
    "            \n",
    "            _html = html.fromstring(response['review_list'])\n",
    "            dates, stars, texts, review_ids, user_ids = [], [], [], [], []\n",
    "            dates = _html.xpath(\"//div[@class='review-content']/descendant::span[@class='rating-qualifier']/text()\")\n",
    "            try:\n",
    "                # datetime conversion causes problems for scraped data with blank dates\n",
    "                dates = [datetime.strptime(d.strip(), format(\"%m/%d/%Y\")) for d in dates]\n",
    "                stars = _html.xpath(\"//div[@class='review-content']/descendant::div[@class='biz-rating__stars']/div/@title\")\n",
    "                stars = [float(s.split(' ')[0]) for s in stars]\n",
    "                texts = [e.text for e in _html.xpath(\"//div[@class='review-content']/p\")]\n",
    "                review_ids = _html.xpath(\"//div[@class='review review--with-sidebar']/@data-review-id\")\n",
    "                user_ids = [s.split(':')[1] for s in _html.xpath(\"//div[@class='review review--with-sidebar']/@data-signup-object\")]\n",
    "                results = [[date, star, text, review_id, user_id] \n",
    "                            for date, star, text, review_id, user_id \n",
    "                            in zip(dates, stars, texts, review_ids, user_ids)]\n",
    "                # let blank dates pass through, resulting in all review dates \n",
    "                # not being formatted in datetime\n",
    "            except ValueError:\n",
    "                stars = _html.xpath(\"//div[@class='review-content']/descendant::div[@class='biz-rating__stars']/div/@title\")\n",
    "                stars = [float(s.split(' ')[0]) for s in stars]\n",
    "                texts = [e.text for e in _html.xpath(\"//div[@class='review-content']/p\")]\n",
    "                review_ids = _html.xpath(\"//div[@class='review review--with-sidebar']/@data-review-id\")\n",
    "                user_ids = [s.split(':')[1] for s in _html.xpath(\"//div[@class='review review--with-sidebar']/@data-signup-object\")]\n",
    "                results = [[date, star, text, review_id, user_id] \n",
    "                            for date, star, text, review_id, user_id \n",
    "                            in zip(dates, stars, texts, review_ids, user_ids)]\n",
    "            \n",
    "            # filter by date\n",
    "            if date_range is not None:\n",
    "                idx0, idx1 = None, None\n",
    "                for i in range(len(dates)):\n",
    "                    if dates[i]<=date_range[1]:\n",
    "                        idx0 = i\n",
    "                        break\n",
    "                for i in range(len(dates)):\n",
    "                    if dates[len(dates)-1-i]>=date_range[0]:\n",
    "                        idx1 = len(dates)-1-i\n",
    "                        break\n",
    "                if idx0 is None or idx1 is None or idx1<idx0: \n",
    "                    results = []\n",
    "                else:\n",
    "                    results = results[idx0:idx1+1]\n",
    "                    keep_scraping = False\n",
    "\n",
    "    return status_code, results, keep_scraping\n",
    "\n",
    "\n",
    "def fixMissingScrapedDates(results):\n",
    "    for i in range(len(results)):\n",
    "        try:\n",
    "            if '/' not in results[i][0]:# if the date is missing\n",
    "                try:\n",
    "                    # assign the review date to be the same the \n",
    "                    # same date as the neighboring past review\n",
    "                    results[i][0] = results[i+1][0]\n",
    "                except IndexError:\n",
    "                    try:\n",
    "                        # if there isn't a past review, assign\n",
    "                        # the date of a newer review\n",
    "                        results[i][0] = results[i-1][0]\n",
    "                    except IndexError:\n",
    "                        # if there is only one other review, assign \n",
    "                        # the earliest review\n",
    "                        results[i][0] = results[0][0]\n",
    "        except TypeError:# if the date is not missing, do nothing\n",
    "            pass\n",
    "    return results\n",
    "    \n",
    "def dbConnect(results, business_id):\n",
    "    item_count = 0\n",
    "    with conn.cursor() as cur:\n",
    "        for i in results:\n",
    "            dateReview = i[0]\n",
    "            try: \n",
    "                dateReview = datetime.strftime(dateReview, \"%Y-%m-%d\")\n",
    "            except TypeError:\n",
    "                dateReview = str(i[0])\n",
    "                dateReview = dateReview.strip()\n",
    "                dateReview = datetime.strptime(dateReview, \"%m/%d/%Y\")\n",
    "            stars = float(i[1])\n",
    "            reviewText = i[2]\n",
    "            reviewId = i[3]\n",
    "            userId = i[4]\n",
    "            cur.execute(\"select distinct business_id from lab.yelp_scraping;\")\n",
    "            \n",
    "            \n",
    "            cur.execute(sql.SQL(\"insert into {} (review_id, business_id, user_id, stars, datetime, date, time, text, timestamp) values ( %s, %s, %s, %s, %s, %s, %s, %s, %s);\")\n",
    "                        .format(sql.Identifier('lab','yelp_scraping')),\n",
    "                        [#'uuid-ossp',# auto generates new uuid\n",
    "                         reviewId,# review id\n",
    "                         business_id,\n",
    "                         userId, # user id\n",
    "                         stars, # stars\n",
    "                         datetime.now(), #local time\n",
    "                         dateReview,# date of review\n",
    "                         datetime.now(),# time without timezone,\n",
    "                         reviewText,# review text\n",
    "                         datetime.now(pytz.utc),# time with timezone\n",
    "                         ])\n",
    "\n",
    "            conn.commit()\n",
    "            cur.execute(\"select * from lab.yelp_scraping\")\n",
    "            for row in cur:\n",
    "                item_count += 1\n",
    "                logger.info(row)\n",
    "        conn.commit()\n",
    "    return \"Added %d items from RDS postGreSQL table\" %(item_count)\n",
    "\n",
    "#normal dates\n",
    "# business_id = \"rR5Y9mp2Yob3rgetJscPWQ\"\n",
    "\n",
    "#missing date examples\n",
    "business_id = \"lLO8Nj-kPJ_b6vEs022GxQ\"\n",
    "business_id = \"fLsXOkjewq5BqQbuUPYC9g\"\n",
    "results = yelpScrapePage(business_id)\n",
    "results = fixMissingScrapedDates(results[1])\n",
    "dbConnect(results, business_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
