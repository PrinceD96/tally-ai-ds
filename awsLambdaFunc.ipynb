{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import Session\n",
    "from lxml import html\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import logging\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import pytz\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#rds settings\n",
    "rds_host  = \"\"\n",
    "name = \"\" \n",
    "db_name = \"\" \n",
    "password = \"\"\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(host=rds_host, user=name, password=password, database=db_name, port=5432, connect_timeout=5)\n",
    "except psycopg2.OperationalError as e:\n",
    "    logger.error(\"ERROR: Unexpected error: Could not connect to postGreSQL instance.\")\n",
    "    logger.error(e)\n",
    "    sys.exit()\n",
    "\n",
    "logger.info(\"SUCCESS: Connection to RDS postGreSQL instance succeeded\")\n",
    "\n",
    "\n",
    "# 2020-01-17 Added review_id, user_id\n",
    "def yelpScraper(business_id):\n",
    "    base_url = \"https://www.yelp.com/biz/\" # add business id\n",
    "    api_url = \"/review_feed?sort_by=date_desc&start=\" # add number\n",
    "\n",
    "    results = []\n",
    "    for n in range(1):\n",
    "        with Session() as s:\n",
    "            url = base_url + business_id + api_url + str(n*20)    \n",
    "            with s.get(url, timeout=5) as r:    \n",
    "                if r.status_code==200:\n",
    "                    response = dict(r.json()) \n",
    "                    _html = html.fromstring(response['review_list']) \n",
    "                    dates = _html.xpath(\"//div[@class='review-content']/descendant::span[@class='rating-qualifier']/text()\")\n",
    "                    try:\n",
    "                        dates = [datetime.strptime(d.strip(), format(\"%m/%d/%Y\")) for d in dates]\n",
    "                        stars = _html.xpath(\"//div[@class='review-content']/descendant::div[@class='biz-rating__stars']/div/@title\")\n",
    "                        stars = [float(s.split(' ')[0]) for s in stars]\n",
    "                        texts = [e.text for e in _html.xpath(\"//div[@class='review-content']/p\")]\n",
    "                        review_ids = _html.xpath(\"//div[@class='review review--with-sidebar']/@data-review-id\")\n",
    "                        user_ids = [s.split(':')[1] for s in _html.xpath(\"//div[@class='review review--with-sidebar']/@data-signup-object\")]\n",
    "                        results = results + [[date, star, text, review_id, user_id] \n",
    "                            for date, star, text, review_id, user_id in zip(dates, stars, texts, review_ids, user_ids)]\n",
    "                    except ValueError:\n",
    "                        stars = _html.xpath(\"//div[@class='review-content']/descendant::div[@class='biz-rating__stars']/div/@title\")\n",
    "                        stars = [float(s.split(' ')[0]) for s in stars]\n",
    "                        texts = [e.text for e in _html.xpath(\"//div[@class='review-content']/p\")]\n",
    "                        review_ids = _html.xpath(\"//div[@class='review review--with-sidebar']/@data-review-id\")\n",
    "                        user_ids = [s.split(':')[1] for s in _html.xpath(\"//div[@class='review review--with-sidebar']/@data-signup-object\")]\n",
    "                        results = results + [[date, star, text, review_id, user_id] \n",
    "                            for date, star, text, review_id, user_id in zip(dates, stars, texts, review_ids, user_ids)]\n",
    "        time.sleep(random.uniform(0.1, 0.5))    \n",
    "    return results\n",
    "\n",
    "\n",
    "def fixMissingScrapedDates(results):# replace missing dates with surrounding dates\n",
    "    for i in range(len(results)):\n",
    "        try:\n",
    "            if '/' not in results[i][0]:\n",
    "                try:\n",
    "                    results[i][0] = results[i+1][0]\n",
    "                except IndexError:\n",
    "                    try:\n",
    "                        results[i][0] = results[i-1][0]\n",
    "                    except IndexError:\n",
    "                        results[i][0] = results[0][0]\n",
    "        except TypeError:\n",
    "            pass\n",
    "    return results\n",
    "    \n",
    "def dbConnect(results, business_id):\n",
    "    item_count = 0\n",
    "    with conn.cursor() as cur:\n",
    "        for i in results:\n",
    "            dateReview = i[0]\n",
    "            try: \n",
    "                # formatting for successfully scraped accounts\n",
    "                dateReview = datetime.strftime(dateReview, \"%Y-%m-%d\")\n",
    "            except TypeError:\n",
    "                # formatting for problem dates that fall through the scraper \n",
    "                # and are still a scraper object/lmtree\n",
    "                dateReview = str(i[0])\n",
    "                dateReview = dateReview.strip()\n",
    "                dateReview = datetime.strptime(dateReview, \"%m/%d/%Y\")\n",
    "            stars = float(i[1])\n",
    "            reviewText = i[2]\n",
    "            reviewId = i[3]\n",
    "            userId = i[4]\n",
    "            cur.execute(\"select distinct business_id from lab.yelp_scraping;\")\n",
    "            \n",
    "            \n",
    "            cur.execute(sql.SQL(\"insert into {} (review_id, business_id, user_id, stars, datetime, date, time, text, timestamp) values ( %s, %s, %s, %s, %s, %s, %s, %s, %s);\")\n",
    "                        .format(sql.Identifier('lab','yelp_scraping')),\n",
    "                        [#'uuid-ossp',#uuid\n",
    "                         reviewId,# review id\n",
    "                         business_id,\n",
    "                         userId, # user id\n",
    "                         stars, # stars\n",
    "                         datetime.now(), # datetime\n",
    "                         dateReview,# date of review\n",
    "                         datetime.now(),# time without timezone,\n",
    "                         reviewText,# review text\n",
    "                         datetime.now(pytz.utc),# time with timezone\n",
    "                         ])\n",
    "\n",
    "            conn.commit()\n",
    "            cur.execute(\"select * from lab.yelp_scraping\")\n",
    "            for row in cur:\n",
    "                item_count += 1\n",
    "                logger.info(row)\n",
    "        conn.commit()\n",
    "    return \"Added %d items from RDS postGreSQL table\" %(item_count)\n",
    "\n",
    "#normal dates\n",
    "# business_id = \"rR5Y9mp2Yob3rgetJscPWQ\"\n",
    "\n",
    "#missing date examples\n",
    "business_id = \"lLO8Nj-kPJ_b6vEs022GxQ\"\n",
    "business_id = \"fLsXOkjewq5BqQbuUPYC9g\"\n",
    "results = yelpScraper(business_id)\n",
    "results = fixMissingScrapedDates(results)\n",
    "dbConnect(results, business_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
